{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6904d94a",
   "metadata": {},
   "source": [
    "This is an example of a simple CNN developed, trained and utilized\n",
    "\n",
    "AI was used to help generate the codebase\n",
    "\n",
    "Note: Make sure that the tensorflow package is installed in your device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35c8ad95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lib imports\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cf51c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET DIRECTORY CONFIGURATION\n",
    "# Download and unzip the dataset from Kaggle, set the directory paths accordingly.\n",
    "train_dir = \"train\"  # e.g. './muffin-vs-chihuahua/train'\n",
    "test_dir = \"test\"    # e.g. './muffin-vs-chihuahua/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef4f9d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMAGE PARAMETERS\n",
    "# Used to resize the input images, also will determine the input size of your input layer.\n",
    "IMG_SIZE = (128, 128)\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d350739e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3788 images belonging to 2 classes.\n",
      "Found 945 images belonging to 2 classes.\n",
      "Found 945 images belonging to 2 classes.\n",
      "Found 1184 images belonging to 2 classes.\n",
      "Found 1184 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# DATA PREPROCESSING & AUGMENTATION\n",
    "# Optional but recommended for image processing tasks, especially with limited data.\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2\n",
    ")\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    subset='training'\n",
    ")\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    subset='validation'\n",
    ")\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f4b1252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPROVED CNN MODEL ARCHITECTURE WITH REGULARIZATION AND DROPOUT\n",
    "\n",
    "# Some modifications are applied\n",
    "initial_learning_rate = 0.001\n",
    "# We are combining ExponentialDecay with Adam optimizer for better learning rate management\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9,\n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "# Create the optimizer with the learning rate schedule\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "# Applied dropout layers to reduce overfitting and L2 regularization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', kernel_regularizer=l2(0.001), input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3)),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71dcbcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the model optimizers, loss function, and metrics\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) # old\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "750c313f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 479ms/step - accuracy: 0.6909 - loss: 0.8006 - val_accuracy: 0.7937 - val_loss: 0.6061\n",
      "Epoch 2/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 1s/step - accuracy: 0.7822 - loss: 0.5755 - val_accuracy: 0.8074 - val_loss: 0.5357\n",
      "Epoch 3/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 1s/step - accuracy: 0.8139 - loss: 0.5034 - val_accuracy: 0.8603 - val_loss: 0.4157\n",
      "Epoch 4/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 1s/step - accuracy: 0.8266 - loss: 0.4586 - val_accuracy: 0.8730 - val_loss: 0.4078\n",
      "Epoch 5/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 1s/step - accuracy: 0.8347 - loss: 0.4585 - val_accuracy: 0.8571 - val_loss: 0.4374\n",
      "Epoch 6/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 1s/step - accuracy: 0.8411 - loss: 0.4298 - val_accuracy: 0.8603 - val_loss: 0.4027\n",
      "Epoch 7/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 1s/step - accuracy: 0.8456 - loss: 0.4199 - val_accuracy: 0.8540 - val_loss: 0.4262\n",
      "Epoch 8/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 1s/step - accuracy: 0.8493 - loss: 0.4248 - val_accuracy: 0.9048 - val_loss: 0.3091\n",
      "Epoch 9/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 969ms/step - accuracy: 0.8519 - loss: 0.4044 - val_accuracy: 0.8847 - val_loss: 0.3445\n",
      "Epoch 10/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 528ms/step - accuracy: 0.8585 - loss: 0.4017 - val_accuracy: 0.8772 - val_loss: 0.3554\n"
     ]
    }
   ],
   "source": [
    "# TRAINING THE CNN\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=val_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7541833a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 309ms/step - accuracy: 0.8226 - loss: 0.4535\n",
      "Test Accuracy: 0.8226351141929626\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 309ms/step - accuracy: 0.8226 - loss: 0.4535\n",
      "Test Accuracy: 0.8226351141929626\n"
     ]
    }
   ],
   "source": [
    "# EVALUATE THE MODEL\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(f\"Test Accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ad7d399",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# SAVE THE IMPROVED MODEL\n",
    "model.save('exercise_6_trained_model_improved.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45472d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIMPLE INFERENCE SCRIPT\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "def predict_image(img_path, model_path='exercise_6_trained_model_improved.h5'):\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    img = image.load_img(img_path, target_size=IMG_SIZE)\n",
    "    img_array = image.img_to_array(img) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    pred = model.predict(img_array)[0,0]\n",
    "    label = \"Chihuahua\" if pred >= 0.5 else \"Muffin\"\n",
    "    print(f\"Prediction: {label} (confidence: {pred:.4f})\")\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b340f1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RUN 1 - Chihuahua Image:\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 902ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 902ms/step\n",
      "Prediction: Muffin (confidence: 0.4317)\n",
      "\n",
      "============================================================\n",
      "RUN 2 - Muffin Image:\n",
      "============================================================\n",
      "Prediction: Muffin (confidence: 0.4317)\n",
      "\n",
      "============================================================\n",
      "RUN 2 - Muffin Image:\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step\n",
      "Prediction: Chihuahua (confidence: 0.8546)\n",
      "Prediction: Chihuahua (confidence: 0.8546)\n"
     ]
    }
   ],
   "source": [
    "# Test predictions on sample images\n",
    "print(\"=\" * 60)\n",
    "print(\"RUN 1 - Chihuahua Image:\")\n",
    "print(\"=\" * 60)\n",
    "pred1 = predict_image(\"test/chihuahua/img_0_1071.jpg\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RUN 2 - Muffin Image:\")\n",
    "print(\"=\" * 60)\n",
    "pred2 = predict_image(\"test/muffin/img_0_0.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b94d09",
   "metadata": {},
   "source": [
    "## Results Summary\n",
    "\n",
    "### Improvements Applied to CNN:\n",
    "1. **Dropout Layers**: Added dropout of 0.25 after each convolutional block and 0.5 before the output layer\n",
    "2. **L2 Regularization**: Applied kernel_regularizer=l2(0.001) to all Conv2D and Dense layers\n",
    "\n",
    "### Results:\n",
    "\n",
    "**Accuracy:**\n",
    "- **Test Accuracy: 82.26%**\n",
    "- Validation Accuracy: 87.72% (final epoch)\n",
    "- Training completed in 10 epochs\n",
    "\n",
    "**Predictions:**\n",
    "- **Run 1 (Chihuahua Image)**: Predicted as **Muffin** with confidence **0.4317** (43.17%) - Incorrect\n",
    "- **Run 2 (Muffin Image)**: Predicted as **Chihuahua** with confidence **0.8546** (85.46%) - Incorrect\n",
    "\n",
    "**Note:** The model with regularization and dropout shows lower accuracy (82.26%) compared to the baseline model (90.29%). This is because dropout and regularization reduce overfitting but may require more epochs or different hyperparameters to achieve better performance. The model is more generalized but needs more training time to reach optimal accuracy.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
