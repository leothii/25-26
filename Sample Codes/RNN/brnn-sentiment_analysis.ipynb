{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b07564aa",
   "metadata": {},
   "source": [
    "A simple implementation of Bidirectional RNN for sentiment analysis\n",
    "\n",
    "Code from Geekforgeeks: https://www.geeksforgeeks.org/deep-learning/bidirectional-recurrent-neural-network/\n",
    "\n",
    "Python packages to import\n",
    "tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d3b63e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# We will use the IMDB dataset built in Keras\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "features = 2000  # Number of most frequent words to consider/time window\n",
    "max_len = 50     # Maximum length of each sequence\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=features)\n",
    "\n",
    "# Data Preprocessing: Padding sequences to ensure uniform input size\n",
    "# Separating the words from the document into sequences of fixed length\n",
    "X_train = pad_sequences(X_train, maxlen=max_len)\n",
    "X_test = pad_sequences(X_test, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17408884",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Bidirectional, SimpleRNN, Dense\n",
    "\n",
    "embedding_dim = 128  \n",
    "hidden_units = 64    \n",
    "\n",
    "model = Sequential()\n",
    "# Embedding() layer maps input features to dense vectors of size embedding (128), with an input length of len.\n",
    "model.add(Embedding(features, embedding_dim, input_length=max_len))\n",
    "\n",
    "# Bidirectional(SimpleRNN(hidden)) adds a bidirectional RNN layer with hidden (64) units.\n",
    "model.add(Bidirectional(SimpleRNN(hidden_units)))\n",
    "\n",
    "# Dense(1, activation='sigmoid') adds a dense output layer with 1 unit and a sigmoid activation for binary classification.\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d57ea44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 13ms/step - accuracy: 0.7242 - loss: 0.5376 - val_accuracy: 0.7779 - val_loss: 0.4858\n",
      "Epoch 2/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.8066 - loss: 0.4218 - val_accuracy: 0.7944 - val_loss: 0.4581\n",
      "Epoch 3/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 13ms/step - accuracy: 0.8389 - loss: 0.3648 - val_accuracy: 0.7468 - val_loss: 0.5277\n",
      "Epoch 4/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 13ms/step - accuracy: 0.8823 - loss: 0.2852 - val_accuracy: 0.7620 - val_loss: 0.5574\n",
      "Epoch 5/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 13ms/step - accuracy: 0.9242 - loss: 0.1968 - val_accuracy: 0.7559 - val_loss: 0.6619\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x235583ddbe0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc53ee2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7559 - loss: 0.6619\n",
      "Test accuracy: 0.7558799982070923\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcf1d85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.75      0.76      0.76     12500\n",
      "    Positive       0.76      0.75      0.75     12500\n",
      "\n",
      "    accuracy                           0.76     25000\n",
      "   macro avg       0.76      0.76      0.76     25000\n",
      "weighted avg       0.76      0.76      0.76     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use test data for predictions\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "y_pred = (y_pred > 0.5)\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be064c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "\u001b[1m1641221/1641221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1us/step\n",
      "\u001b[1m1641221/1641221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1us/step\n",
      "Testing BRNN Model on Sample Strings:\n",
      "\n",
      "------------------------------------------------------------\n",
      "Testing BRNN Model on Sample Strings:\n",
      "\n",
      "------------------------------------------------------------\n",
      "Text: The movie was fantastic! I really loved it.\n",
      "Prediction: Positive (Confidence: 0.9322)\n",
      "------------------------------------------------------------\n",
      "Text: The movie was fantastic! I really loved it.\n",
      "Prediction: Positive (Confidence: 0.9322)\n",
      "------------------------------------------------------------\n",
      "Text: This film is terrible and boring. Waste of time.Text: This film is terrible and boring. Waste of time.\n",
      "Prediction: Positive (Confidence: 0.9907)\n",
      "------------------------------------------------------------\n",
      "\n",
      "Prediction: Positive (Confidence: 0.9907)\n",
      "------------------------------------------------------------\n",
      "Text: Amazing performance by the actors. Highly recommended!\n",
      "Prediction: Positive (Confidence: 0.9892)\n",
      "------------------------------------------------------------\n",
      "Text: Worst movie I've ever seen. Absolutely horrible.\n",
      "Prediction: Positive (Confidence: 0.9721)\n",
      "------------------------------------------------------------\n",
      "Text: Amazing performance by the actors. Highly recommended!\n",
      "Prediction: Positive (Confidence: 0.9892)\n",
      "------------------------------------------------------------\n",
      "Text: Worst movie I've ever seen. Absolutely horrible.\n",
      "Prediction: Positive (Confidence: 0.9721)\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test the model on sample strings\n",
    "from keras.datasets import imdb\n",
    "\n",
    "# Get the word index from IMDB dataset\n",
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "# Decode function to convert encoded text back to words\n",
    "# because the IMDB dataset encodes words as integers\n",
    "def decode_review(encoded_text):\n",
    "    \"\"\"Convert encoded text back to words\"\"\"\n",
    "    return ' '.join([reverse_word_index.get(i - 3, '?') for i in encoded_text])\n",
    "\n",
    "# Encode function to convert text to indices\n",
    "# Which the BRNN model can process\n",
    "def encode_text(text):\n",
    "    \"\"\"Convert text string to encoded indices\"\"\"\n",
    "    words = text.lower().split()\n",
    "    encoded = []\n",
    "    for word in words:\n",
    "        if word in word_index and word_index[word] < features:\n",
    "            encoded.append(word_index[word])\n",
    "    return encoded\n",
    "\n",
    "# Test samples\n",
    "# You can modify these strings to test different inputs\n",
    "test_samples = [\n",
    "    \"The movie was fantastic! I really loved it.\",\n",
    "    \"This film is terrible and boring. Waste of time.\",\n",
    "    \"Amazing performance by the actors. Highly recommended!\",\n",
    "    \"Worst movie I've ever seen. Absolutely horrible.\"\n",
    "]\n",
    "\n",
    "print(\"Testing BRNN Model on Sample Strings:\\n\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for test_text in test_samples:\n",
    "    # Encode the text\n",
    "    encoded = encode_text(test_text)\n",
    "    \n",
    "    # Pad the sequence\n",
    "    padded = pad_sequences([encoded], maxlen=max_len)\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict(padded, verbose=0)\n",
    "    sentiment = \"Positive\" if prediction[0][0] > 0.5 else \"Negative\"\n",
    "    confidence = prediction[0][0] if prediction[0][0] > 0.5 else 1 - prediction[0][0]\n",
    "    \n",
    "    print(f\"Text: {test_text}\")\n",
    "    print(f\"Prediction: {sentiment} (Confidence: {confidence:.4f})\")\n",
    "    print(\"-\" * 60)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
