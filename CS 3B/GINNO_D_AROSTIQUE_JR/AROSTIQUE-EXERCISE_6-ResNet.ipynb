{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e55f540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIBRARY IMPORTS\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Flatten, Dropout\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "775b533e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET DIRECTORY CONFIGURATION\n",
    "\n",
    "# Define the paths to the training and testing datasets\n",
    "# Ensure the dataset is properly downloaded and unzipped before running the code\n",
    "test_dir = r\"C:\\Users\\USER\\OneDrive\\Desktop\\Codes\\muffin-vs-chihuahua\\test\"\n",
    "train_dir = r\"C:\\Users\\USER\\OneDrive\\Desktop\\Codes\\muffin-vs-chihuahua\\train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6be3ba7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMAGE PARAMETERS\n",
    "\n",
    "# Define the image size for resizing and the batch size for data loading\n",
    "IMG_SIZE = (224, 224)  # Resize all images to 224x224 pixels for ResNet50\n",
    "BATCH_SIZE = 32  # Number of images to process in a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "737e10fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3788 images belonging to 2 classes.\n",
      "Found 945 images belonging to 2 classes.\n",
      "Found 1184 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# DATA PREPROCESSING & AUGMENTATION\n",
    "\n",
    "# Apply data augmentation to the training dataset to improve model generalization\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,  # Normalize pixel values to [0, 1]\n",
    "    rotation_range=15,  # Randomly rotate images by up to 15 degrees\n",
    "    width_shift_range=0.1,  # Randomly shift images horizontally by 10%\n",
    "    height_shift_range=0.1,  # Randomly shift images vertically by 10%\n",
    "    horizontal_flip=True,  # Randomly flip images horizontally\n",
    "    validation_split=0.2  # Reserve 20% of training data for validation\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)  # Only normalize test data\n",
    "\n",
    "# Create data generators for training, validation, and testing datasets\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    subset='training',  # Use the training subset\n",
    ")\n",
    "\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    subset='validation',  # Use the validation subset\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    shuffle=False  # Do not shuffle test data to maintain order\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83e40822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESNET50 MODEL ARCHITECTURE\n",
    "initial_learning_rate = 0.001\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9,\n",
    "    staircase=True\n",
    ")\n",
    "optimizer = Adam(learning_rate=lr_schedule)\n",
    "IMG_SIZE = (224, 224)  # ResNet50 default input size\n",
    "resnet_base = ResNet50(weights='imagenet', include_top=False, input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
    "# Freeze the ResNet50 base layers\n",
    "for layer in resnet_base.layers:\n",
    "    layer.trainable = False\n",
    "# Add custom layers on top of ResNet50\n",
    "x = Flatten()(resnet_base.output)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output_layer = Dense(1, activation='sigmoid')(x)  # Binary output\n",
    "model = Model(inputs=resnet_base.input, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcd6014b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURE THE MODEL\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7abd3607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m217s\u001b[0m 2s/step - accuracy: 0.6360 - loss: 1.3935 - val_accuracy: 0.6878 - val_loss: 0.5551\n",
      "Epoch 2/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 1s/step - accuracy: 0.6573 - loss: 0.6007 - val_accuracy: 0.7524 - val_loss: 0.5741\n",
      "Epoch 3/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 2s/step - accuracy: 0.6983 - loss: 0.5811 - val_accuracy: 0.7556 - val_loss: 0.4999\n",
      "Epoch 4/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 1s/step - accuracy: 0.6940 - loss: 0.5823 - val_accuracy: 0.8063 - val_loss: 0.4767\n",
      "Epoch 5/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 1s/step - accuracy: 0.7210 - loss: 0.5665 - val_accuracy: 0.8212 - val_loss: 0.4476\n",
      "Epoch 6/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 1s/step - accuracy: 0.6856 - loss: 0.5622 - val_accuracy: 0.7937 - val_loss: 0.4943\n",
      "Epoch 7/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 1s/step - accuracy: 0.6534 - loss: 0.5936 - val_accuracy: 0.7270 - val_loss: 0.5448\n",
      "Epoch 8/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m713s\u001b[0m 6s/step - accuracy: 0.6581 - loss: 0.5894 - val_accuracy: 0.8286 - val_loss: 0.4513\n",
      "Epoch 9/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m482s\u001b[0m 4s/step - accuracy: 0.7297 - loss: 0.5479 - val_accuracy: 0.8169 - val_loss: 0.4056\n",
      "Epoch 10/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m560s\u001b[0m 5s/step - accuracy: 0.7397 - loss: 0.5346 - val_accuracy: 0.8254 - val_loss: 0.4289\n"
     ]
    }
   ],
   "source": [
    "# TRAINING THE RESNET MODEL\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=val_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f08f717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 4s/step - accuracy: 0.8218 - loss: 0.4361\n",
      "Test accuracy: 0.8218\n"
     ]
    }
   ],
   "source": [
    "# EVALUATE THE RESNET MODEL\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "273cd3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as 'exercise_6_arostique_ResNet.h5'\n"
     ]
    }
   ],
   "source": [
    "# SAVE THE RESNET MODEL\n",
    "model.save('exercise_6_arostique_ResNet.h5')\n",
    "print(\"Model saved as 'exercise_6_arostique_ResNet.h5'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e6d1a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIMPLE INFERENCE SCRIPT\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "def predict_image(img_path, model_path='exercise_6_arostique_ResNet.h5'):\n",
    "    # Load the trained model\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "    # Load and preprocess the image\n",
    "    img = image.load_img(img_path, target_size=(224, 224))  # Ensure this matches model input\n",
    "    img_array = image.img_to_array(img) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "    # Make a prediction\n",
    "    pred = model.predict(img_array)[0, 0]\n",
    "    label = \"Muffin\" if pred >= 0.5 else \"Chihuahua\"\n",
    "    print(f\"Prediction: {label} (confidence: {pred:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c38d4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n",
      "Prediction: Chihuahua (confidence: 0.33)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n",
      "Prediction: Muffin (confidence: 0.68)\n"
     ]
    }
   ],
   "source": [
    "# EXAMPLE USAGE\n",
    "\n",
    "# Test the model with example images\n",
    "predict_image(r\"C:\\Users\\USER\\OneDrive\\Desktop\\Codes\\muffin-vs-chihuahua\\run\\run_1.jpeg\")\n",
    "predict_image(r\"C:\\Users\\USER\\OneDrive\\Desktop\\Codes\\muffin-vs-chihuahua\\run\\run_2.jpg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
