{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0c0eb39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.20.0\n",
      "ResNet50 ready for transfer learning\n"
     ]
    }
   ],
   "source": [
    "# Lib imports\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "import numpy as np\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"ResNet50 ready for transfer learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f1f8b26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET DIRECTORY CONFIGURATION\n",
    "# Download and unzip the dataset from Kaggle, set the directory paths accordingly.\n",
    "train_dir = r\"C:\\Users\\Admin\\Downloads\\cvr_train\"  # e.g. './muffin-vs-chihuahua/train'\n",
    "test_dir = r\"C:\\Users\\Admin\\Downloads\\cvr_test\"    # e.g. './muffin-vs-chihuahua/test'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a64cca",
   "metadata": {},
   "source": [
    "## ğŸ±ğŸ° New Dataset: Cats vs Rabbits Classification\n",
    "\n",
    "This notebook has been adapted to work with a **Cats vs Rabbits** binary classification dataset.\n",
    "\n",
    "**Changes from previous dataset (Muffin vs Chihuahua):**\n",
    "- Dataset paths updated to `cvr_train` and `cvr_test`\n",
    "- Same CNN architecture with regularization and dropout\n",
    "- Binary classification: Cat (class 0) vs Rabbit (class 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8e87e44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CATS vs RABBITS DATASET VERIFICATION\n",
      "======================================================================\n",
      "âœ… Directories found!\n",
      "   Train: C:\\Users\\Admin\\Downloads\\cvr_train\n",
      "   Test:  C:\\Users\\Admin\\Downloads\\cvr_test\n",
      "\n",
      "======================================================================\n",
      "DATASET STRUCTURE AND STATISTICS\n",
      "======================================================================\n",
      "\n",
      "ğŸ“ Training Directory Classes: ['cat', 'rabbit']\n",
      "   â€¢ Cat: 801 images\n",
      "   â€¢ Rabbit: 800 images\n",
      "   â€¢ TOTAL: 1,601 images\n",
      "\n",
      "ğŸ“ Test Directory Classes: ['cat', 'rabbit']\n",
      "   â€¢ Cat: 11 images\n",
      "   â€¢ Rabbit: 5 images\n",
      "   â€¢ TOTAL: 16 images\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š DATASET FEEDBACK\n",
      "======================================================================\n",
      "â€¢ Train/Test Split: 99.0% / 1.0%\n",
      "  âš ï¸  High training ratio - consider larger test set\n",
      "\n",
      "â€¢ Class Balance Ratio: 1.00\n",
      "  âœ… Well-balanced dataset\n",
      "\n",
      "======================================================================\n",
      "ğŸ’¡ RECOMMENDATIONS\n",
      "======================================================================\n",
      "â€¢ Image augmentation is enabled (rotation, shifts, flips)\n",
      "â€¢ Using 20% validation split for model evaluation\n",
      "â€¢ L2 regularization and dropout are applied to prevent overfitting\n",
      "â€¢ Ready to proceed with training!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# DATASET VERIFICATION AND FEEDBACK\n",
    "import os\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CATS vs RABBITS DATASET VERIFICATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check if directories exist\n",
    "if not os.path.exists(train_dir):\n",
    "    print(f\"âŒ ERROR: Training directory not found!\")\n",
    "    print(f\"   Expected path: {train_dir}\")\n",
    "    print(f\"   Please create the directory with subdirectories: 'cat' and 'rabbit'\")\n",
    "elif not os.path.exists(test_dir):\n",
    "    print(f\"âŒ ERROR: Test directory not found!\")\n",
    "    print(f\"   Expected path: {test_dir}\")\n",
    "    print(f\"   Please create the directory with subdirectories: 'cat' and 'rabbit'\")\n",
    "else:\n",
    "    print(f\"âœ… Directories found!\")\n",
    "    print(f\"   Train: {train_dir}\")\n",
    "    print(f\"   Test:  {test_dir}\")\n",
    "    \n",
    "    # Show dataset structure and statistics\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"DATASET STRUCTURE AND STATISTICS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Training data\n",
    "    if os.path.exists(train_dir):\n",
    "        subdirs = [d for d in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, d))]\n",
    "        print(f\"\\nğŸ“ Training Directory Classes: {subdirs}\")\n",
    "        \n",
    "        total_train = 0\n",
    "        for subdir in subdirs:\n",
    "            path = os.path.join(train_dir, subdir)\n",
    "            num_files = len([f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))])\n",
    "            total_train += num_files\n",
    "            print(f\"   â€¢ {subdir.capitalize()}: {num_files:,} images\")\n",
    "        print(f\"   â€¢ TOTAL: {total_train:,} images\")\n",
    "    \n",
    "    # Test data\n",
    "    if os.path.exists(test_dir):\n",
    "        subdirs = [d for d in os.listdir(test_dir) if os.path.isdir(os.path.join(test_dir, d))]\n",
    "        print(f\"\\nğŸ“ Test Directory Classes: {subdirs}\")\n",
    "        \n",
    "        total_test = 0\n",
    "        for subdir in subdirs:\n",
    "            path = os.path.join(test_dir, subdir)\n",
    "            num_files = len([f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))])\n",
    "            total_test += num_files\n",
    "            print(f\"   â€¢ {subdir.capitalize()}: {num_files:,} images\")\n",
    "        print(f\"   â€¢ TOTAL: {total_test:,} images\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ğŸ“Š DATASET FEEDBACK\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Calculate split ratio\n",
    "    if total_train > 0 and total_test > 0:\n",
    "        train_pct = (total_train / (total_train + total_test)) * 100\n",
    "        test_pct = (total_test / (total_train + total_test)) * 100\n",
    "        print(f\"â€¢ Train/Test Split: {train_pct:.1f}% / {test_pct:.1f}%\")\n",
    "        \n",
    "        if 70 <= train_pct <= 85:\n",
    "            print(f\"  âœ… Good split ratio for model training\")\n",
    "        elif train_pct < 70:\n",
    "            print(f\"  âš ï¸  Low training data - may need more training images\")\n",
    "        else:\n",
    "            print(f\"  âš ï¸  High training ratio - consider larger test set\")\n",
    "    \n",
    "    # Class balance check\n",
    "    if os.path.exists(train_dir):\n",
    "        subdirs = [d for d in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, d))]\n",
    "        counts = []\n",
    "        for subdir in subdirs:\n",
    "            path = os.path.join(train_dir, subdir)\n",
    "            counts.append(len([f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]))\n",
    "        \n",
    "        if len(counts) == 2:\n",
    "            balance_ratio = min(counts) / max(counts)\n",
    "            print(f\"\\nâ€¢ Class Balance Ratio: {balance_ratio:.2f}\")\n",
    "            if balance_ratio >= 0.8:\n",
    "                print(f\"  âœ… Well-balanced dataset\")\n",
    "            elif balance_ratio >= 0.6:\n",
    "                print(f\"  âš ï¸  Slightly imbalanced - consider data augmentation\")\n",
    "            else:\n",
    "                print(f\"  âŒ Imbalanced dataset - may cause bias toward majority class\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ğŸ’¡ RECOMMENDATIONS\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"â€¢ Image augmentation is enabled (rotation, shifts, flips)\")\n",
    "    print(\"â€¢ Using 20% validation split for model evaluation\")\n",
    "    print(\"â€¢ L2 regularization and dropout are applied to prevent overfitting\")\n",
    "    print(\"â€¢ Ready to proceed with training!\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e02e4447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMAGE PARAMETERS\n",
    "# Used to resize the input images, also will determine the input size of your input layer.\n",
    "IMG_SIZE = (128, 128)\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8ac51d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DATA PREPROCESSING - RESNET50 COMPATIBLE\n",
      "======================================================================\n",
      "Found 1280 images belonging to 2 classes.\n",
      "Found 320 images belonging to 2 classes.\n",
      "Found 320 images belonging to 2 classes.\n",
      "Found 15 images belonging to 2 classes.\n",
      "\n",
      "âœ… Data generators created with ResNet50 preprocessing\n",
      "   â€¢ Training samples: 1280\n",
      "   â€¢ Validation samples: 320\n",
      "   â€¢ Test samples: 15\n",
      "======================================================================\n",
      "Found 15 images belonging to 2 classes.\n",
      "\n",
      "âœ… Data generators created with ResNet50 preprocessing\n",
      "   â€¢ Training samples: 1280\n",
      "   â€¢ Validation samples: 320\n",
      "   â€¢ Test samples: 15\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# DATA PREPROCESSING & AUGMENTATION FOR RESNET50\n",
    "# ResNet50 requires specific preprocessing (mean subtraction, not rescaling to 0-1)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DATA PREPROCESSING - RESNET50 COMPATIBLE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# For ResNet50, we use preprocessing_function instead of rescale\n",
    "# ResNet50 expects inputs in range [0, 255] with specific mean subtraction\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,  # ResNet50 preprocessing\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    zoom_range=0.2,\n",
    "    validation_split=0.2\n",
    ")\n",
    "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    subset='training'\n",
    ")\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    subset='validation'\n",
    ")\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Data generators created with ResNet50 preprocessing\")\n",
    "print(f\"   â€¢ Training samples: {train_generator.samples}\")\n",
    "print(f\"   â€¢ Validation samples: {val_generator.samples}\")\n",
    "print(f\"   â€¢ Test samples: {test_generator.samples}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04f83e5",
   "metadata": {},
   "source": [
    "## ğŸ—ï¸ ResNet50 Architecture\n",
    "\n",
    "**What is ResNet50?**\n",
    "- **Residual Network** with 50 layers deep\n",
    "- Introduced by Microsoft Research in 2015\n",
    "- Uses **skip connections** (residual connections) to solve vanishing gradient problem\n",
    "- Pre-trained on **ImageNet** (1.4M images, 1000 classes)\n",
    "\n",
    "**Why use ResNet50?**\n",
    "- âœ… **Transfer Learning**: Leverages pre-trained features from ImageNet\n",
    "- âœ… **Better Accuracy**: Deep architecture captures complex patterns\n",
    "- âœ… **Faster Training**: Frozen base layers reduce training time\n",
    "- âœ… **Less Data Needed**: Pre-trained features work well with smaller datasets\n",
    "\n",
    "**Our Implementation:**\n",
    "- **Base**: ResNet50 (frozen) - feature extraction\n",
    "- **Custom Head**: Classification layers for cats vs rabbits\n",
    "- **Regularization**: Dropout + L2 to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c301b2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "BUILDING RESNET50 MODEL FOR CATS vs RABBITS\n",
      "======================================================================\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m94765736/94765736\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 0us/step\n",
      "\u001b[1m94765736/94765736\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 0us/step\n",
      "\n",
      "âœ… ResNet50 base loaded with ImageNet weights\n",
      "   Base model layers: 175\n",
      "   Base model trainable: False\n",
      "\n",
      "ğŸ“Š Model Architecture:\n",
      "\n",
      "âœ… ResNet50 base loaded with ImageNet weights\n",
      "   Base model layers: 175\n",
      "   Base model trainable: False\n",
      "\n",
      "ğŸ“Š Model Architecture:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ resnet50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)     â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling2d        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,544</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ resnet50 (\u001b[38;5;33mFunctional\u001b[0m)           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m2048\u001b[0m)     â”‚    \u001b[38;5;34m23,587,712\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling2d        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_10 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            â”‚       \u001b[38;5;34m524,544\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_9 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_11 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚        \u001b[38;5;34m32,896\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_10 (\u001b[38;5;33mDropout\u001b[0m)            â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_12 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              â”‚           \u001b[38;5;34m129\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24,145,281</span> (92.11 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m24,145,281\u001b[0m (92.11 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">557,569</span> (2.13 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m557,569\u001b[0m (2.13 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> (89.98 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m23,587,712\u001b[0m (89.98 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ğŸ”§ TRANSFER LEARNING CONFIGURATION:\n",
      "======================================================================\n",
      "â€¢ Base Model: ResNet50 (frozen)\n",
      "â€¢ Pre-trained Weights: ImageNet\n",
      "â€¢ Custom Head: GlobalAvgPool â†’ Dense(256) â†’ Dropout(0.5) â†’ Dense(128) â†’ Dropout(0.3) â†’ Output\n",
      "â€¢ Regularization: L2 (0.001) + Dropout (0.5, 0.3)\n",
      "â€¢ Optimizer: Adam with ExponentialDecay\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# RESNET50 CNN MODEL ARCHITECTURE WITH TRANSFER LEARNING\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"BUILDING RESNET50 MODEL FOR CATS vs RABBITS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Learning rate configuration\n",
    "initial_learning_rate = 0.001\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9,\n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "# Create the optimizer with the learning rate schedule\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "# Load pre-trained ResNet50 (without top classification layer)\n",
    "# Using ImageNet weights for transfer learning\n",
    "base_model = ResNet50(\n",
    "    weights='imagenet',  # Pre-trained on ImageNet\n",
    "    include_top=False,   # Exclude final classification layer\n",
    "    input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3)\n",
    ")\n",
    "\n",
    "# Freeze the base model layers (transfer learning)\n",
    "base_model.trainable = False\n",
    "print(f\"\\nâœ… ResNet50 base loaded with ImageNet weights\")\n",
    "print(f\"   Base model layers: {len(base_model.layers)}\")\n",
    "print(f\"   Base model trainable: {base_model.trainable}\")\n",
    "\n",
    "# Build custom top layers for binary classification\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "model = models.Sequential([\n",
    "    # ResNet50 base\n",
    "    base_model,\n",
    "    \n",
    "    # Custom classification head with regularization\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(256, activation='relu', \n",
    "                 kernel_regularizer=regularizers.l2(0.001)),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(0.001)),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(1, activation='sigmoid')  # Binary classification\n",
    "])\n",
    "\n",
    "print(\"\\nğŸ“Š Model Architecture:\")\n",
    "model.summary()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ”§ TRANSFER LEARNING CONFIGURATION:\")\n",
    "print(\"=\"*70)\n",
    "print(\"â€¢ Base Model: ResNet50 (frozen)\")\n",
    "print(\"â€¢ Pre-trained Weights: ImageNet\")\n",
    "print(\"â€¢ Custom Head: GlobalAvgPool â†’ Dense(256) â†’ Dropout(0.5) â†’ Dense(128) â†’ Dropout(0.3) â†’ Output\")\n",
    "print(\"â€¢ Regularization: L2 (0.001) + Dropout (0.5, 0.3)\")\n",
    "print(\"â€¢ Optimizer: Adam with ExponentialDecay\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "43fcd841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the model optimizers, loss function, and metrics\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) # old\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "61e1e1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 352ms/step - accuracy: 0.8523 - loss: 1.0616 - val_accuracy: 0.9531 - val_loss: 0.7089\n",
      "Epoch 2/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 352ms/step - accuracy: 0.8523 - loss: 1.0616 - val_accuracy: 0.9531 - val_loss: 0.7089\n",
      "Epoch 2/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 303ms/step - accuracy: 0.9383 - loss: 0.7582 - val_accuracy: 0.9656 - val_loss: 0.6403\n",
      "Epoch 3/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 303ms/step - accuracy: 0.9383 - loss: 0.7582 - val_accuracy: 0.9656 - val_loss: 0.6403\n",
      "Epoch 3/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 301ms/step - accuracy: 0.9453 - loss: 0.6867 - val_accuracy: 0.9719 - val_loss: 0.6050\n",
      "Epoch 4/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 301ms/step - accuracy: 0.9453 - loss: 0.6867 - val_accuracy: 0.9719 - val_loss: 0.6050\n",
      "Epoch 4/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 301ms/step - accuracy: 0.9656 - loss: 0.5793 - val_accuracy: 0.9625 - val_loss: 0.5619\n",
      "Epoch 5/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 301ms/step - accuracy: 0.9656 - loss: 0.5793 - val_accuracy: 0.9625 - val_loss: 0.5619\n",
      "Epoch 5/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 302ms/step - accuracy: 0.9719 - loss: 0.5338 - val_accuracy: 0.9688 - val_loss: 0.5204\n",
      "Epoch 6/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 302ms/step - accuracy: 0.9719 - loss: 0.5338 - val_accuracy: 0.9688 - val_loss: 0.5204\n",
      "Epoch 6/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 300ms/step - accuracy: 0.9664 - loss: 0.5200 - val_accuracy: 0.9656 - val_loss: 0.4962\n",
      "Epoch 7/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 300ms/step - accuracy: 0.9664 - loss: 0.5200 - val_accuracy: 0.9656 - val_loss: 0.4962\n",
      "Epoch 7/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 303ms/step - accuracy: 0.9688 - loss: 0.4803 - val_accuracy: 0.9719 - val_loss: 0.4711\n",
      "Epoch 8/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 303ms/step - accuracy: 0.9688 - loss: 0.4803 - val_accuracy: 0.9719 - val_loss: 0.4711\n",
      "Epoch 8/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 307ms/step - accuracy: 0.9742 - loss: 0.4636 - val_accuracy: 0.9750 - val_loss: 0.4267\n",
      "Epoch 9/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 307ms/step - accuracy: 0.9742 - loss: 0.4636 - val_accuracy: 0.9750 - val_loss: 0.4267\n",
      "Epoch 9/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 305ms/step - accuracy: 0.9695 - loss: 0.4371 - val_accuracy: 0.9844 - val_loss: 0.4023\n",
      "Epoch 10/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 305ms/step - accuracy: 0.9695 - loss: 0.4371 - val_accuracy: 0.9844 - val_loss: 0.4023\n",
      "Epoch 10/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 309ms/step - accuracy: 0.9719 - loss: 0.4152 - val_accuracy: 0.9781 - val_loss: 0.3663\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 309ms/step - accuracy: 0.9719 - loss: 0.4152 - val_accuracy: 0.9781 - val_loss: 0.3663\n"
     ]
    }
   ],
   "source": [
    "# TRAINING THE CNN\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=val_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "499fafbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FINAL TRAINING RESULTS (Epoch 10):\n",
      "============================================================\n",
      "Training Accuracy:   0.9719 (97.19%)\n",
      "Validation Accuracy: 0.9781 (97.81%)\n",
      "Training Loss:       0.4152\n",
      "Validation Loss:     0.3663\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# DISPLAY FINAL ACCURACY RESULTS\n",
    "print(\"=\"*60)\n",
    "print(\"FINAL TRAINING RESULTS (Epoch 10):\")\n",
    "print(\"=\"*60)\n",
    "final_train_acc = history.history['accuracy'][-1]\n",
    "final_val_acc = history.history['val_accuracy'][-1]\n",
    "final_train_loss = history.history['loss'][-1]\n",
    "final_val_loss = history.history['val_loss'][-1]\n",
    "\n",
    "print(f\"Training Accuracy:   {final_train_acc:.4f} ({final_train_acc*100:.2f}%)\")\n",
    "print(f\"Validation Accuracy: {final_val_acc:.4f} ({final_val_acc*100:.2f}%)\")\n",
    "print(f\"Training Loss:       {final_train_loss:.4f}\")\n",
    "print(f\"Validation Loss:     {final_val_loss:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "53e84432",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ResNet50 model saved as: cats_vs_rabbits_resnet50.h5\n"
     ]
    }
   ],
   "source": [
    "# SAVE THE RESNET50 MODEL\n",
    "model.save('cats_vs_rabbits_resnet50.h5')\n",
    "print(\"âœ… ResNet50 model saved as: cats_vs_rabbits_resnet50.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d6856080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE SCRIPT FOR CATS VS RABBITS (RESNET50)\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "def predict_image(img_path, model_path='cats_vs_rabbits_resnet50.h5'):\n",
    "    \"\"\"\n",
    "    Predict whether an image is a Cat or Rabbit using ResNet50\n",
    "    \n",
    "    Args:\n",
    "        img_path: Path to the image file\n",
    "        model_path: Path to the trained ResNet50 model\n",
    "    \n",
    "    Returns:\n",
    "        Prints prediction and confidence score\n",
    "    \"\"\"\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    img = image.load_img(img_path, target_size=IMG_SIZE)\n",
    "    img_array = image.img_to_array(img)\n",
    "    \n",
    "    # ResNet50 preprocessing (NOT rescale to 0-1)\n",
    "    img_array = preprocess_input(img_array)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    \n",
    "    pred = model.predict(img_array)[0,0]\n",
    "    \n",
    "    # ImageDataGenerator assigns labels alphabetically: cat=0, rabbit=1\n",
    "    label = \"Rabbit\" if pred >= 0.5 else \"Cat\"\n",
    "    confidence = pred if pred >= 0.5 else (1 - pred)\n",
    "    \n",
    "    print(f\"Prediction: {label} (confidence: {confidence:.2f})\")\n",
    "    return label, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a993a161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PREDICTIONS ON TEST IMAGES - CATS vs RABBITS\n",
      "======================================================================\n",
      "\n",
      "âš ï¸  run_1 folder not found. Using sample images from test set instead:\n",
      "\n",
      "--- Run 1 Prediction (Test Image 1) ---\n",
      "Image path: C:\\Users\\Admin\\Downloads\\cvr_test\\cat\\cat.208.jpg\n",
      "Actual class (from folder): Cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002420A31DB20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002420A31DB20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Prediction: Cat (confidence: 1.00)\n",
      "Result: âœ… CORRECT\n",
      "\n",
      "--- Run 2 Prediction (Test Image 2) ---\n",
      "Image path: C:\\Users\\Admin\\Downloads\\cvr_test\\cat\\cat.209.jpg\n",
      "Actual class (from folder): Cat\n",
      "Prediction: Cat (confidence: 1.00)\n",
      "Result: âœ… CORRECT\n",
      "\n",
      "--- Run 2 Prediction (Test Image 2) ---\n",
      "Image path: C:\\Users\\Admin\\Downloads\\cvr_test\\cat\\cat.209.jpg\n",
      "Actual class (from folder): Cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002420A2E3920> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002420A2E3920> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Prediction: Cat (confidence: 1.00)\n",
      "Result: âœ… CORRECT\n",
      "======================================================================\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Prediction: Cat (confidence: 1.00)\n",
      "Result: âœ… CORRECT\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# PREDICTIONS ON TEST IMAGES (Run 1 and Run 2)\n",
    "import os\n",
    "import glob\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PREDICTIONS ON TEST IMAGES - CATS vs RABBITS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check if run_1 folder exists, otherwise use sample images from test set\n",
    "if os.path.exists(\"run_1/run_1.jpg\") and os.path.exists(\"run_1/run_2.jpg\"):\n",
    "    print(\"\\nâœ… Using images from run_1 folder:\")\n",
    "    print(\"\\n--- Run 1 Prediction ---\")\n",
    "    predict_image(\"run_1/run_1.jpg\")\n",
    "    print(\"\\n--- Run 2 Prediction ---\")\n",
    "    predict_image(\"run_1/run_2.jpg\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  run_1 folder not found. Using sample images from test set instead:\\n\")\n",
    "    \n",
    "    # Get sample images from test set\n",
    "    cat_imgs = glob.glob(os.path.join(test_dir, \"cat*\", \"*.jpg\"))[:1]\n",
    "    rabbit_imgs = glob.glob(os.path.join(test_dir, \"rabbit*\", \"*.jpg\"))[:1]\n",
    "    \n",
    "    if not cat_imgs:\n",
    "        cat_imgs = glob.glob(os.path.join(test_dir, \"*\", \"*.jpg\"))[:1]\n",
    "    if not rabbit_imgs:\n",
    "        rabbit_imgs = glob.glob(os.path.join(test_dir, \"*\", \"*.jpg\"))[1:2]\n",
    "    \n",
    "    if cat_imgs:\n",
    "        print(\"--- Run 1 Prediction (Test Image 1) ---\")\n",
    "        print(f\"Image path: {cat_imgs[0]}\")\n",
    "        actual_class = \"Cat\" if \"cat\" in cat_imgs[0].lower() else \"Rabbit\" if \"rabbit\" in cat_imgs[0].lower() else \"Unknown\"\n",
    "        print(f\"Actual class (from folder): {actual_class}\")\n",
    "        label, conf = predict_image(cat_imgs[0])\n",
    "        result = \"âœ… CORRECT\" if label == actual_class else \"âŒ INCORRECT\"\n",
    "        print(f\"Result: {result}\\n\")\n",
    "    \n",
    "    if rabbit_imgs:\n",
    "        print(\"--- Run 2 Prediction (Test Image 2) ---\")\n",
    "        print(f\"Image path: {rabbit_imgs[0]}\")\n",
    "        actual_class = \"Cat\" if \"cat\" in rabbit_imgs[0].lower() else \"Rabbit\" if \"rabbit\" in rabbit_imgs[0].lower() else \"Unknown\"\n",
    "        print(f\"Actual class (from folder): {actual_class}\")\n",
    "        label, conf = predict_image(rabbit_imgs[0])\n",
    "        result = \"âœ… CORRECT\" if label == actual_class else \"âŒ INCORRECT\"\n",
    "        print(f\"Result: {result}\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "21097ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ğŸ±ğŸ° CATS VS RABBITS - RESNET50 CLASSIFICATION RESULTS\n",
      "======================================================================\n",
      "\n",
      "ğŸ—ï¸  MODEL ARCHITECTURE:\n",
      "   â€¢ Base Model: ResNet50 (50 layers)\n",
      "   â€¢ Pre-trained: ImageNet weights\n",
      "   â€¢ Transfer Learning: Frozen base + custom head\n",
      "   â€¢ Total Parameters: 24,145,281\n",
      "\n",
      "ğŸ“Š MODEL ACCURACY:\n",
      "   â€¢ Training Accuracy:    97.19%\n",
      "   â€¢ Validation Accuracy:  97.81%\n",
      "   â€¢ Training Loss:        0.4152\n",
      "   â€¢ Validation Loss:      0.3663\n",
      "\n",
      "ğŸ“ˆ MODEL PERFORMANCE ANALYSIS:\n",
      "   âœ… Excellent generalization (gap: 0.62%)\n",
      "\n",
      "ğŸ”§ TECHNIQUES APPLIED:\n",
      "   â€¢ Transfer Learning: Pre-trained ResNet50 on ImageNet\n",
      "   â€¢ Frozen Layers: Base model frozen for faster training\n",
      "   â€¢ Dropout: 0.5 and 0.3 in classification head\n",
      "   â€¢ L2 Regularization: 0.001 on Dense layers\n",
      "   â€¢ Data Augmentation: rotation, shifts, zoom, flip\n",
      "   â€¢ Learning Rate Decay: ExponentialDecay (0.001 â†’ 0.9)\n",
      "   â€¢ ResNet Preprocessing: ImageNet mean subtraction\n",
      "\n",
      "ğŸ’¾ MODEL SAVED:\n",
      "   â€¢ Filename: cats_vs_rabbits_resnet50.h5\n",
      "======================================================================\n",
      "\n",
      "ğŸ’¡ ADVANTAGES OF RESNET50:\n",
      "   1. âœ… Deeper network (50 vs 7 layers in simple CNN)\n",
      "   2. âœ… Skip connections prevent vanishing gradients\n",
      "   3. âœ… Pre-trained features from 1.4M ImageNet images\n",
      "   4. âœ… Better feature extraction with less training data\n",
      "   5. âœ… Faster convergence with transfer learning\n",
      "\n",
      "ğŸ“ NEXT STEPS:\n",
      "   1. Run predictions to test the ResNet50 model\n",
      "   2. Compare accuracy with simple CNN baseline\n",
      "   3. Optional: Fine-tune top ResNet layers for better accuracy\n",
      "   4. Optional: Try other architectures (VGG16, InceptionV3)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# FINAL SUMMARY - CATS VS RABBITS WITH RESNET50\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ±ğŸ° CATS VS RABBITS - RESNET50 CLASSIFICATION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nğŸ—ï¸  MODEL ARCHITECTURE:\")\n",
    "print(\"   â€¢ Base Model: ResNet50 (50 layers)\")\n",
    "print(\"   â€¢ Pre-trained: ImageNet weights\")\n",
    "print(\"   â€¢ Transfer Learning: Frozen base + custom head\")\n",
    "print(f\"   â€¢ Total Parameters: {model.count_params():,}\")\n",
    "\n",
    "print(\"\\nğŸ“Š MODEL ACCURACY:\")\n",
    "print(f\"   â€¢ Training Accuracy:    {final_train_acc*100:.2f}%\")\n",
    "print(f\"   â€¢ Validation Accuracy:  {final_val_acc*100:.2f}%\")\n",
    "print(f\"   â€¢ Training Loss:        {final_train_loss:.4f}\")\n",
    "print(f\"   â€¢ Validation Loss:      {final_val_loss:.4f}\")\n",
    "\n",
    "# Calculate if there's overfitting or underfitting\n",
    "gap = abs(final_train_acc - final_val_acc)\n",
    "print(f\"\\nğŸ“ˆ MODEL PERFORMANCE ANALYSIS:\")\n",
    "if gap < 0.05:\n",
    "    print(f\"   âœ… Excellent generalization (gap: {gap*100:.2f}%)\")\n",
    "elif gap < 0.10:\n",
    "    print(f\"   âš ï¸  Slight overfitting (gap: {gap*100:.2f}%)\")\n",
    "else:\n",
    "    print(f\"   âŒ Overfitting detected (gap: {gap*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nğŸ”§ TECHNIQUES APPLIED:\")\n",
    "print(f\"   â€¢ Transfer Learning: Pre-trained ResNet50 on ImageNet\")\n",
    "print(f\"   â€¢ Frozen Layers: Base model frozen for faster training\")\n",
    "print(f\"   â€¢ Dropout: 0.5 and 0.3 in classification head\")\n",
    "print(f\"   â€¢ L2 Regularization: 0.001 on Dense layers\")\n",
    "print(f\"   â€¢ Data Augmentation: rotation, shifts, zoom, flip\")\n",
    "print(f\"   â€¢ Learning Rate Decay: ExponentialDecay (0.001 â†’ 0.9)\")\n",
    "print(f\"   â€¢ ResNet Preprocessing: ImageNet mean subtraction\")\n",
    "\n",
    "print(f\"\\nğŸ’¾ MODEL SAVED:\")\n",
    "print(f\"   â€¢ Filename: cats_vs_rabbits_resnet50.h5\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nğŸ’¡ ADVANTAGES OF RESNET50:\")\n",
    "print(\"   1. âœ… Deeper network (50 vs 7 layers in simple CNN)\")\n",
    "print(\"   2. âœ… Skip connections prevent vanishing gradients\")\n",
    "print(\"   3. âœ… Pre-trained features from 1.4M ImageNet images\")\n",
    "print(\"   4. âœ… Better feature extraction with less training data\")\n",
    "print(\"   5. âœ… Faster convergence with transfer learning\")\n",
    "\n",
    "print(\"\\nğŸ“ NEXT STEPS:\")\n",
    "print(\"   1. Run predictions to test the ResNet50 model\")\n",
    "print(\"   2. Compare accuracy with simple CNN baseline\")\n",
    "print(\"   3. Optional: Fine-tune top ResNet layers for better accuracy\")\n",
    "print(\"   4. Optional: Try other architectures (VGG16, InceptionV3)\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80bcb4d",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ Performance Comparison: Simple CNN vs ResNet50\n",
    "\n",
    "### Results Summary:\n",
    "\n",
    "| Metric | Simple CNN | ResNet50 | Improvement |\n",
    "|--------|------------|----------|-------------|\n",
    "| **Training Accuracy** | 80.39% | 97.19% | +16.80% ğŸš€ |\n",
    "| **Validation Accuracy** | 78.44% | 97.81% | +19.37% ğŸš€ |\n",
    "| **Training Loss** | 0.4765 | 0.4152 | -12.86% âœ… |\n",
    "| **Validation Loss** | 0.4866 | 0.3663 | -24.72% âœ… |\n",
    "| **Generalization Gap** | 1.95% | 0.62% | Better âœ… |\n",
    "| **Training Time** | ~5-6 min | ~2 min | Faster âœ… |\n",
    "| **Parameters** | 3.3M | 24.1M (557K trainable) | Transfer Learning |\n",
    "| **Test Predictions** | Mixed | 100% Correct | Perfect âœ… |\n",
    "\n",
    "### ğŸ¯ Key Takeaways:\n",
    "\n",
    "1. **Massive Accuracy Improvement**: ResNet50 achieved **97.81%** vs Simple CNN's **78.44%**\n",
    "   - That's a **19.37% absolute improvement**!\n",
    "   \n",
    "2. **Better Generalization**: ResNet50 has only **0.62% gap** between train/val (vs 1.95%)\n",
    "   - Less overfitting despite deeper architecture\n",
    "   \n",
    "3. **Perfect Predictions**: Both test images classified correctly with **1.00 confidence**\n",
    "   - Simple CNN struggled with test images\n",
    "   \n",
    "4. **Transfer Learning Wins**: Pre-trained ImageNet features are highly effective\n",
    "   - Only 557K parameters need training (vs 23.5M frozen)\n",
    "   \n",
    "5. **Faster Training**: ResNet50 converged in ~2 minutes despite being deeper\n",
    "   - Frozen layers + pre-trained weights = faster convergence\n",
    "\n",
    "### ğŸ’¡ When to Use Each:\n",
    "\n",
    "**Use Simple CNN when:**\n",
    "- âœ… Very limited computational resources\n",
    "- âœ… Need tiny model size for deployment\n",
    "- âœ… Training time is critical constraint\n",
    "- âœ… Dataset is very small (<500 images)\n",
    "\n",
    "**Use ResNet50 when:**\n",
    "- âœ… Need high accuracy (production systems)\n",
    "- âœ… Have moderate dataset (500+ images)\n",
    "- âœ… Can afford larger model size\n",
    "- âœ… **Want best results** â† Most cases!\n",
    "\n",
    "### ğŸ† Verdict: ResNet50 is the clear winner for this task!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4757e49",
   "metadata": {},
   "source": [
    "## ğŸ“Š ResNet50 Model Summary\n",
    "\n",
    "### ğŸ—ï¸ Architecture Highlights:\n",
    "- **Model**: ResNet50 (Residual Network with 50 layers)\n",
    "- **Transfer Learning**: Pre-trained on ImageNet (1.4M images)\n",
    "- **Base Layers**: Frozen (23.5M parameters)\n",
    "- **Custom Head**: 2 Dense layers with Dropout\n",
    "- **Innovation**: Skip connections solve vanishing gradient problem\n",
    "\n",
    "### âœ… Advantages over Simple CNN:\n",
    "1. **Deeper Architecture**: 50 layers vs 7 layers\n",
    "2. **Pre-trained Features**: Leverages ImageNet knowledge\n",
    "3. **Better Accuracy**: Expected 5-15% improvement\n",
    "4. **Faster Training**: Frozen base reduces computation\n",
    "5. **Less Overfitting**: Transfer learning with small datasets\n",
    "\n",
    "### ğŸ¯ Expected Performance:\n",
    "- **Simple CNN**: ~78-88% validation accuracy\n",
    "- **ResNet50**: ~85-95% validation accuracy (typical improvement)\n",
    "\n",
    "### âš ï¸ Dataset Reminder:\n",
    "- **Training**: 1,601 images (perfectly balanced)\n",
    "- **Test Set**: Only 16 images (needs expansion for reliable metrics)\n",
    "\n",
    "### \udd04 Comparison Points:\n",
    "| Feature | Simple CNN | ResNet50 |\n",
    "|---------|-----------|----------|\n",
    "| Layers | 7 | 50 |\n",
    "| Parameters | 3.3M | ~25M |\n",
    "| Training Time | Faster | Slower (first epoch) |\n",
    "| Accuracy | Good | Better |\n",
    "| Pre-training | None | ImageNet |\n",
    "\n",
    "### ğŸ’¡ When to Use ResNet50:\n",
    "- âœ… Small to medium datasets\n",
    "- âœ… Need high accuracy\n",
    "- âœ… Transfer learning scenarios\n",
    "- âœ… Limited training time/resources"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
