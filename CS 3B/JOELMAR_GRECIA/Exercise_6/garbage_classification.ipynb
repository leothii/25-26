{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6904d94a",
   "metadata": {},
   "source": [
    "This is an example of a simple CNN developed, trained and utilized\n",
    "\n",
    "AI was used to help generate the codebase\n",
    "\n",
    "Note: Make sure that the tensorflow package is installed in your device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "35c8ad95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lib imports\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6cf51c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET DIRECTORY CONFIGURATION\n",
    "# Using garbage_classification dataset (paper vs plastic)\n",
    "train_dir = \"garbage_classification/train\"  \n",
    "test_dir = \"garbage_classification/test\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ef4f9d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMAGE PARAMETERS\n",
    "# Used to resize the input images, also will determine the input size of your input layer.\n",
    "IMG_SIZE = (128, 128)\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d350739e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1532 images belonging to 2 classes.\n",
      "Found 383 images belonging to 2 classes.\n",
      "Found 1915 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# DATA PREPROCESSING & AUGMENTATION\n",
    "# Optional but recommended for image processing tasks, especially with limited data.\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2\n",
    ")\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    subset='training'\n",
    ")\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    subset='validation'\n",
    ")\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3f4b1252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPROVED CNN MODEL ARCHITECTURE WITH REGULARIZATION AND DROPOUT\n",
    "\n",
    "# Some modifications are applied\n",
    "initial_learning_rate = 0.001\n",
    "# We are combining ExponentialDecay with Adam optimizer for better learning rate management\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9,\n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "# Create the optimizer with the learning rate schedule\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "# Applied dropout layers and L2 regularization to reduce overfitting\n",
    "# L2 regularization helps prevent overfitting by penalizing large weights\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3)),\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "71dcbcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the model optimizers, loss function, and metrics\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) # old\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "750c313f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 1s/step - accuracy: 0.5521 - loss: 1.1863 - val_accuracy: 0.6762 - val_loss: 0.9034\n",
      "Epoch 2/10\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 493ms/step - accuracy: 0.6315 - loss: 0.8354 - val_accuracy: 0.8146 - val_loss: 0.6376\n",
      "Epoch 3/10\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 559ms/step - accuracy: 0.7116 - loss: 0.7183 - val_accuracy: 0.7311 - val_loss: 0.6216\n",
      "Epoch 4/10\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 502ms/step - accuracy: 0.7809 - loss: 0.6036 - val_accuracy: 0.8120 - val_loss: 0.6664\n",
      "Epoch 5/10\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 515ms/step - accuracy: 0.7315 - loss: 0.6255 - val_accuracy: 0.8407 - val_loss: 0.5104\n",
      "Epoch 6/10\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 494ms/step - accuracy: 0.7813 - loss: 0.5385 - val_accuracy: 0.8486 - val_loss: 0.4399\n",
      "Epoch 7/10\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 511ms/step - accuracy: 0.8051 - loss: 0.4729 - val_accuracy: 0.8460 - val_loss: 0.4135\n",
      "Epoch 8/10\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 521ms/step - accuracy: 0.8262 - loss: 0.4386 - val_accuracy: 0.8433 - val_loss: 0.4049\n",
      "Epoch 9/10\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 521ms/step - accuracy: 0.7982 - loss: 0.4729 - val_accuracy: 0.7963 - val_loss: 0.4857\n",
      "Epoch 10/10\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 516ms/step - accuracy: 0.8054 - loss: 0.4454 - val_accuracy: 0.7807 - val_loss: 0.4800\n"
     ]
    }
   ],
   "source": [
    "# TRAINING THE CNN\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=val_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7541833a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 595ms/step - accuracy: 0.9140 - loss: 0.2828\n",
      "\n",
      "==================================================\n",
      "IMPROVED MODEL - Test Accuracy: 0.7232 (72.32%)\n",
      "Test Loss: 0.6300\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# EVALUATE THE MODEL\n",
    "test_loss, test_acc = model.evaluate(test_generator, verbose=1)\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"IMPROVED MODEL - Test Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2ad7d399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved model saved as 'muffin_vs_chihuahua_cnn_improved.keras'\n"
     ]
    }
   ],
   "source": [
    "# SAVE THE IMPROVED MODEL\n",
    "model.save('muffin_vs_chihuahua_cnn_improved.keras')\n",
    "print(\"Improved model saved as 'muffin_vs_chihuahua_cnn_improved.keras'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "45472d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE SCRIPT FOR IMPROVED MODEL\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# Load model once for efficiency\n",
    "_loaded_model = None\n",
    "\n",
    "def predict_image(img_path, model_path='muffin_vs_chihuahua_cnn_improved.keras', img_size=IMG_SIZE):\n",
    "    global _loaded_model\n",
    "    if _loaded_model is None:\n",
    "        _loaded_model = tf.keras.models.load_model(model_path)\n",
    "    img = image.load_img(img_path, target_size=img_size)\n",
    "    img_array = image.img_to_array(img) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    pred = _loaded_model.predict(img_array, verbose=0)[0,0]\n",
    "    label = \"Plastic\" if pred >= 0.5 else \"Paper\"  # Class 0=Paper, Class 1=Plastic (alphabetical order)\n",
    "    confidence = pred if pred >= 0.5 else (1 - pred)\n",
    "    print(f\"Image: {img_path}\")\n",
    "    print(f\"Prediction: {label}\")\n",
    "    print(f\"Confidence: {confidence:.4f} ({confidence*100:.2f}%)\")\n",
    "    print(f\"Raw prediction value: {pred:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "    return label, pred, confidence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b340f1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: run_1/run_1.jpg\n",
      "Prediction: Paper\n",
      "Confidence: 0.9900 (99.00%)\n",
      "Raw prediction value: 0.0100\n",
      "--------------------------------------------------\n",
      "Image: run_1/run_2.jpg\n",
      "Prediction: Paper\n",
      "Confidence: 0.9936 (99.36%)\n",
      "Raw prediction value: 0.0064\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Paper', np.float32(0.0063803317), np.float32(0.9936197))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_image(\"run_1/run_1.jpg\")\n",
    "predict_image(\"run_1/run_2.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f9af7165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved model architecture created with:\n",
      "- L2 regularization (lambda=0.001) on all Conv2D and Dense layers\n",
      "- Dropout layers: 0.25 after conv layers, 0.5 after dense layer\n"
     ]
    }
   ],
   "source": [
    "# IMPROVED CNN MODEL WITH REGULARIZATION AND DROPOUT\n",
    "\n",
    "\n",
    "# Learning rate schedule\n",
    "initial_learning_rate = 0.001\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9,\n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "# Create the optimizer with the learning rate schedule\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "# Improved model with dropout and L2 regularization\n",
    "# L2 regularization (lambda=0.001) penalizes large weights to prevent overfitting\n",
    "# Dropout layers randomly deactivate neurons during training to improve generalization\n",
    "model_improved = models.Sequential([\n",
    "    layers.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3)),\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the improved model\n",
    "model_improved.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"Improved model architecture created with:\")\n",
    "print(\"- L2 regularization (lambda=0.001) on all Conv2D and Dense layers\")\n",
    "print(\"- Dropout layers: 0.25 after conv layers, 0.5 after dense layer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8dde7c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training improved model with regularization and dropout...\n",
      "Epoch 1/10\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 580ms/step - accuracy: 0.4975 - loss: 1.3240 - val_accuracy: 0.5483 - val_loss: 0.9144\n",
      "Epoch 2/10\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 562ms/step - accuracy: 0.5604 - loss: 0.8804 - val_accuracy: 0.5979 - val_loss: 0.8346\n",
      "Epoch 3/10\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 520ms/step - accuracy: 0.6863 - loss: 0.7416 - val_accuracy: 0.7885 - val_loss: 0.6410\n",
      "Epoch 4/10\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 517ms/step - accuracy: 0.7409 - loss: 0.6384 - val_accuracy: 0.8303 - val_loss: 0.5018\n",
      "Epoch 5/10\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 527ms/step - accuracy: 0.7518 - loss: 0.5903 - val_accuracy: 0.8146 - val_loss: 0.4641\n",
      "Epoch 6/10\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 524ms/step - accuracy: 0.8072 - loss: 0.5127 - val_accuracy: 0.7781 - val_loss: 0.5287\n",
      "Epoch 7/10\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 590ms/step - accuracy: 0.8082 - loss: 0.4698 - val_accuracy: 0.8251 - val_loss: 0.4011\n",
      "Epoch 8/10\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 567ms/step - accuracy: 0.8006 - loss: 0.4771 - val_accuracy: 0.8303 - val_loss: 0.4013\n",
      "Epoch 9/10\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 562ms/step - accuracy: 0.8292 - loss: 0.4497 - val_accuracy: 0.8538 - val_loss: 0.4023\n",
      "Epoch 10/10\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 638ms/step - accuracy: 0.8404 - loss: 0.4249 - val_accuracy: 0.8407 - val_loss: 0.3958\n"
     ]
    }
   ],
   "source": [
    "# TRAINING THE IMPROVED CNN MODEL\n",
    "print(\"Training improved model with regularization and dropout...\")\n",
    "history_improved = model_improved.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=val_generator,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d5f4d40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATING IMPROVED MODEL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 153ms/step - accuracy: 0.9277 - loss: 0.2681\n",
      "IMPROVED MODEL RESULTS:\n",
      "Test Accuracy: 0.7864 (78.64%)\n",
      "Test Loss: 0.5251\n",
      "2a) ACCURACY: \n",
      "The improved model achieves a test accuracy of 0.7864 (78.64%)\n"
     ]
    }
   ],
   "source": [
    "# EVALUATE THE IMPROVED MODEL\n",
    "print(\"EVALUATING IMPROVED MODEL\")\n",
    "test_loss_improved, test_acc_improved = model_improved.evaluate(test_generator, verbose=1)\n",
    "print(f\"IMPROVED MODEL RESULTS:\")\n",
    "print(f\"Test Accuracy: {test_acc_improved:.4f} ({test_acc_improved*100:.2f}%)\")\n",
    "print(f\"Test Loss: {test_loss_improved:.4f}\")\n",
    "\n",
    "#2a: Accuracy\n",
    "print(\"2a) ACCURACY: \")\n",
    "print(f\"The improved model achieves a test accuracy of {test_acc_improved:.4f} ({test_acc_improved*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ba007da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved model saved as 'exercise_6_custom_lastname.grecia.h5'\n"
     ]
    }
   ],
   "source": [
    "# SAVE THE IMPROVED MODEL\n",
    "model_improved.save('exercise_6_custom_grecia.h5')\n",
    "print(\"Improved model saved as 'exercise_6_custom_lastname.grecia.h5'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e2191cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPROVED MODEL PREDICTIONS ON RUN 1 AND RUN 2\n",
      "\n",
      "Run 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Image: run_1/run_1.jpg\n",
      "  Prediction: Plastic\n",
      "  Confidence: 0.8273 (82.73%)\n",
      "  Raw prediction value: 0.8273\n",
      "\n",
      "Run 2:\n",
      "  Image: run_1/run_2.jpg\n",
      "  Prediction: Plastic\n",
      "  Confidence: 0.5565 (55.65%)\n",
      "  Raw prediction value: 0.5565\n",
      "2b): RUN 1 AND 2 PREDICTIONS\n",
      "Run 1 Prediction: Plastic\n",
      "Run 1 Confidence: 0.8273 (82.73%)\n",
      "\n",
      "Run 2 Prediction: Plastic\n",
      "Run 2 Confidence: 0.5565 (55.65%)\n"
     ]
    }
   ],
   "source": [
    "# PREDICTIONS ON RUN 1 AND RUN 2 USING IMPROVED MODEL\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# Load the improved model\n",
    "_loaded_model_improved = None\n",
    "\n",
    "def predict_image_improved(img_path, model_path='exercise_6_trained_model_improved.h5', img_size=IMG_SIZE):\n",
    "    global _loaded_model_improved\n",
    "    if _loaded_model_improved is None:\n",
    "        _loaded_model_improved = tf.keras.models.load_model(model_path)\n",
    "    img = image.load_img(img_path, target_size=img_size)\n",
    "    img_array = image.img_to_array(img) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    pred = _loaded_model_improved.predict(img_array, verbose=0)[0,0]\n",
    "    label = \"Plastic\" if pred >= 0.5 else \"Paper\" \n",
    "    confidence = pred if pred >= 0.5 else (1 - pred)\n",
    "    return label, pred, confidence\n",
    "\n",
    "print(\"IMPROVED MODEL PREDICTIONS ON RUN 1 AND RUN 2\")\n",
    "\n",
    "print(\"\\nRun 1:\")\n",
    "label1, pred1, conf1 = predict_image_improved(\"run_1/run_1.jpg\")\n",
    "print(f\"  Image: run_1/run_1.jpg\")\n",
    "print(f\"  Prediction: {label1}\")\n",
    "print(f\"  Confidence: {conf1:.4f} ({conf1*100:.2f}%)\")\n",
    "print(f\"  Raw prediction value: {pred1:.4f}\")\n",
    "\n",
    "print(\"\\nRun 2:\")\n",
    "label2, pred2, conf2 = predict_image_improved(\"run_1/run_2.jpg\")\n",
    "print(f\"  Image: run_1/run_2.jpg\")\n",
    "print(f\"  Prediction: {label2}\")\n",
    "print(f\"  Confidence: {conf2:.4f} ({conf2*100:.2f}%)\")\n",
    "print(f\"  Raw prediction value: {pred2:.4f}\")\n",
    "\n",
    "print(\"2b): RUN 1 AND 2 PREDICTIONS\")\n",
    "print(f\"Run 1 Prediction: {label1}\")\n",
    "print(f\"Run 1 Confidence: {conf1:.4f} ({conf1*100:.2f}%)\")\n",
    "print(f\"\\nRun 2 Prediction: {label2}\")\n",
    "print(f\"Run 2 Confidence: {conf2:.4f} ({conf2*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a328f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
