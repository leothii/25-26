{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c272707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library imports\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models, Model\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61d541e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Dataset directories configured successfully\n",
      "       >> Train directory: ./archive/train\n",
      "       >> Test directory: ./archive/test\n"
     ]
    }
   ],
   "source": [
    "# Dataset directory configuration\n",
    "TRAIN_DIR = \"./archive/train\"\n",
    "TEST_DIR = \"./archive/test\"\n",
    "PREDICT_DIR = \"./archive/predict\"\n",
    "\n",
    "# Verify directories exist\n",
    "if not os.path.exists(TRAIN_DIR):\n",
    "    raise ValueError(f\"[ERROR] Training directory not found: {TRAIN_DIR}\")\n",
    "if not os.path.exists(TEST_DIR):\n",
    "    raise ValueError(f\"[ERROR] Test directory not found: {TEST_DIR}\")\n",
    "\n",
    "print(f\"[INFO] Dataset directories configured successfully\")\n",
    "print(f\"       >> Train directory: {TRAIN_DIR}\")\n",
    "print(f\"       >> Test directory: {TEST_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b88af208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Hyperparameters configured:\n",
      "       >> Image size: (224, 224)\n",
      "       >> Batch size: 32\n",
      "       >> Epochs: 15\n",
      "       >> Learning rate: 0.0001\n",
      "       >> Validation split: 0.2\n"
     ]
    }
   ],
   "source": [
    "# Image parameters and hyperparameters\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "IMG_SIZE = (IMG_HEIGHT, IMG_WIDTH)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 15\n",
    "LEARNING_RATE = 0.0001\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "print(f\"[INFO] Hyperparameters configured:\")\n",
    "print(f\"       >> Image size: {IMG_SIZE}\")\n",
    "print(f\"       >> Batch size: {BATCH_SIZE}\")\n",
    "print(f\"       >> Epochs: {EPOCHS}\")\n",
    "print(f\"       >> Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"       >> Validation split: {VALIDATION_SPLIT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fb4b527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3788 images belonging to 2 classes.\n",
      "Found 945 images belonging to 2 classes.\n",
      "Found 945 images belonging to 2 classes.\n",
      "Found 1184 images belonging to 2 classes.\n",
      "[INFO] Data generators created successfully\n",
      "       >> Training samples: 3788\n",
      "       >> Validation samples: 945\n",
      "       >> Test samples: 1184\n",
      "       >> Class indices: {'chihuahua': 0, 'muffin': 1}\n",
      "Found 1184 images belonging to 2 classes.\n",
      "[INFO] Data generators created successfully\n",
      "       >> Training samples: 3788\n",
      "       >> Validation samples: 945\n",
      "       >> Test samples: 1184\n",
      "       >> Class indices: {'chihuahua': 0, 'muffin': 1}\n"
     ]
    }
   ],
   "source": [
    "# Data preprocessing and augmentation\n",
    "# Training data with augmentation to prevent overfitting\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255.0,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=VALIDATION_SPLIT\n",
    ")\n",
    "\n",
    "# Test data with only rescaling (no augmentation)\n",
    "test_datagen = ImageDataGenerator(rescale=1.0 / 255.0)\n",
    "\n",
    "# Create data generators\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    subset='training',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    subset='validation',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    TEST_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"[INFO] Data generators created successfully\")\n",
    "print(f\"       >> Training samples: {train_generator.samples}\")\n",
    "print(f\"       >> Validation samples: {validation_generator.samples}\")\n",
    "print(f\"       >> Test samples: {test_generator.samples}\")\n",
    "print(f\"       >> Class indices: {train_generator.class_indices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6243946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ResNet model built successfully\n",
      "       >> Total layers: 9\n",
      "       >> Base model trainable: False\n"
     ]
    }
   ],
   "source": [
    "# ResNet CNN model architecture\n",
    "def build_resnet_model(img_height, img_width, learning_rate):\n",
    "    \"\"\"\n",
    "    Build an optimized ResNet-based CNN model for binary classification.\n",
    "    \n",
    "    Uses transfer learning with ResNet50 pre-trained on ImageNet.\n",
    "    The base model is fine-tuned with custom top layers.\n",
    "    \n",
    "    Args:\n",
    "        img_height (int): Height of input images\n",
    "        img_width (int): Width of input images\n",
    "        learning_rate (float): Learning rate for optimizer\n",
    "    \n",
    "    Returns:\n",
    "        Model: Compiled Keras model\n",
    "    \"\"\"\n",
    "    # Load pre-trained ResNet50 without top layers\n",
    "    base_model = ResNet50(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=(img_height, img_width, 3)\n",
    "    )\n",
    "    \n",
    "    # Freeze the base model initially\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    # Build custom top layers\n",
    "    inputs = layers.Input(shape=(img_height, img_width, 3))\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    # Create the complete model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    # Compile model with optimizer and loss function\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    "    )\n",
    "    \n",
    "    print(f\"[INFO] ResNet model built successfully\")\n",
    "    print(f\"       >> Total layers: {len(model.layers)}\")\n",
    "    print(f\"       >> Base model trainable: {base_model.trainable}\")\n",
    "    \n",
    "    return model, base_model\n",
    "\n",
    "# Build the model\n",
    "model, base_model = build_resnet_model(IMG_HEIGHT, IMG_WIDTH, LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dda30503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Model Architecture Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ resnet50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,192</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,544</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_3 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ resnet50 (\u001b[38;5;33mFunctional\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m2048\u001b[0m)     │    \u001b[38;5;34m23,587,712\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_1      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │         \u001b[38;5;34m8,192\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m524,544\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24,153,473</span> (92.14 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m24,153,473\u001b[0m (92.14 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">561,665</span> (2.14 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m561,665\u001b[0m (2.14 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,591,808</span> (90.00 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m23,591,808\u001b[0m (90.00 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display model architecture summary\n",
    "print(\"[INFO] Model Architecture Summary:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "524a40f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Training callbacks configured\n",
      "       >> Early stopping: patience=5\n",
      "       >> Learning rate reduction: factor=0.5, patience=3\n"
     ]
    }
   ],
   "source": [
    "# Configure training callbacks for optimization\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "callbacks_list = [early_stopping, reduce_lr]\n",
    "\n",
    "print(f\"[INFO] Training callbacks configured\")\n",
    "print(f\"       >> Early stopping: patience={early_stopping.patience}\")\n",
    "print(f\"       >> Learning rate reduction: factor={reduce_lr.factor}, patience={reduce_lr.patience}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "379fe0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting training - Phase 1: Frozen base model\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m469s\u001b[0m 4s/step - accuracy: 0.6988 - loss: 0.5942 - precision_1: 0.6720 - recall_1: 0.6724 - val_accuracy: 0.4963 - val_loss: 0.7351 - val_precision_1: 0.4769 - val_recall_1: 1.0000 - learning_rate: 1.0000e-04\n",
      "Epoch 2/15\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m469s\u001b[0m 4s/step - accuracy: 0.6988 - loss: 0.5942 - precision_1: 0.6720 - recall_1: 0.6724 - val_accuracy: 0.4963 - val_loss: 0.7351 - val_precision_1: 0.4769 - val_recall_1: 1.0000 - learning_rate: 1.0000e-04\n",
      "Epoch 2/15\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m313s\u001b[0m 3s/step - accuracy: 0.7416 - loss: 0.5377 - precision_1: 0.7272 - recall_1: 0.7000 - val_accuracy: 0.7249 - val_loss: 0.5750 - val_precision_1: 0.6355 - val_recall_1: 0.9401 - learning_rate: 1.0000e-04\n",
      "Epoch 3/15\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m313s\u001b[0m 3s/step - accuracy: 0.7416 - loss: 0.5377 - precision_1: 0.7272 - recall_1: 0.7000 - val_accuracy: 0.7249 - val_loss: 0.5750 - val_precision_1: 0.6355 - val_recall_1: 0.9401 - learning_rate: 1.0000e-04\n",
      "Epoch 3/15\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m371s\u001b[0m 3s/step - accuracy: 0.7640 - loss: 0.5096 - precision_1: 0.7515 - recall_1: 0.7264 - val_accuracy: 0.8116 - val_loss: 0.4826 - val_precision_1: 0.7667 - val_recall_1: 0.8479 - learning_rate: 1.0000e-04\n",
      "Epoch 4/15\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m371s\u001b[0m 3s/step - accuracy: 0.7640 - loss: 0.5096 - precision_1: 0.7515 - recall_1: 0.7264 - val_accuracy: 0.8116 - val_loss: 0.4826 - val_precision_1: 0.7667 - val_recall_1: 0.8479 - learning_rate: 1.0000e-04\n",
      "Epoch 4/15\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 2s/step - accuracy: 0.7685 - loss: 0.4971 - precision_1: 0.7522 - recall_1: 0.7397 - val_accuracy: 0.8349 - val_loss: 0.4339 - val_precision_1: 0.8248 - val_recall_1: 0.8134 - learning_rate: 1.0000e-04\n",
      "Epoch 5/15\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 2s/step - accuracy: 0.7685 - loss: 0.4971 - precision_1: 0.7522 - recall_1: 0.7397 - val_accuracy: 0.8349 - val_loss: 0.4339 - val_precision_1: 0.8248 - val_recall_1: 0.8134 - learning_rate: 1.0000e-04\n",
      "Epoch 5/15\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m303s\u001b[0m 3s/step - accuracy: 0.7656 - loss: 0.4951 - precision_1: 0.7412 - recall_1: 0.7523 - val_accuracy: 0.8265 - val_loss: 0.4125 - val_precision_1: 0.8590 - val_recall_1: 0.7442 - learning_rate: 1.0000e-04\n",
      "Epoch 6/15\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m303s\u001b[0m 3s/step - accuracy: 0.7656 - loss: 0.4951 - precision_1: 0.7412 - recall_1: 0.7523 - val_accuracy: 0.8265 - val_loss: 0.4125 - val_precision_1: 0.8590 - val_recall_1: 0.7442 - learning_rate: 1.0000e-04\n",
      "Epoch 6/15\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m291s\u001b[0m 2s/step - accuracy: 0.7785 - loss: 0.4859 - precision_1: 0.7709 - recall_1: 0.7368 - val_accuracy: 0.8392 - val_loss: 0.3771 - val_precision_1: 0.8490 - val_recall_1: 0.7903 - learning_rate: 1.0000e-04\n",
      "Epoch 7/15\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m291s\u001b[0m 2s/step - accuracy: 0.7785 - loss: 0.4859 - precision_1: 0.7709 - recall_1: 0.7368 - val_accuracy: 0.8392 - val_loss: 0.3771 - val_precision_1: 0.8490 - val_recall_1: 0.7903 - learning_rate: 1.0000e-04\n",
      "Epoch 7/15\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m521s\u001b[0m 4s/step - accuracy: 0.7809 - loss: 0.4741 - precision_1: 0.7695 - recall_1: 0.7466 - val_accuracy: 0.8275 - val_loss: 0.3831 - val_precision_1: 0.8501 - val_recall_1: 0.7581 - learning_rate: 1.0000e-04\n",
      "Epoch 8/15\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m521s\u001b[0m 4s/step - accuracy: 0.7809 - loss: 0.4741 - precision_1: 0.7695 - recall_1: 0.7466 - val_accuracy: 0.8275 - val_loss: 0.3831 - val_precision_1: 0.8501 - val_recall_1: 0.7581 - learning_rate: 1.0000e-04\n",
      "Epoch 8/15\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m288s\u001b[0m 2s/step - accuracy: 0.7793 - loss: 0.4738 - precision_1: 0.7697 - recall_1: 0.7414 - val_accuracy: 0.8339 - val_loss: 0.3605 - val_precision_1: 0.8437 - val_recall_1: 0.7834 - learning_rate: 1.0000e-04\n",
      "Epoch 9/15\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m288s\u001b[0m 2s/step - accuracy: 0.7793 - loss: 0.4738 - precision_1: 0.7697 - recall_1: 0.7414 - val_accuracy: 0.8339 - val_loss: 0.3605 - val_precision_1: 0.8437 - val_recall_1: 0.7834 - learning_rate: 1.0000e-04\n",
      "Epoch 9/15\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m300s\u001b[0m 3s/step - accuracy: 0.7827 - loss: 0.4674 - precision_1: 0.7747 - recall_1: 0.7431 - val_accuracy: 0.8466 - val_loss: 0.3518 - val_precision_1: 0.8622 - val_recall_1: 0.7926 - learning_rate: 1.0000e-04\n",
      "Epoch 10/15\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m300s\u001b[0m 3s/step - accuracy: 0.7827 - loss: 0.4674 - precision_1: 0.7747 - recall_1: 0.7431 - val_accuracy: 0.8466 - val_loss: 0.3518 - val_precision_1: 0.8622 - val_recall_1: 0.7926 - learning_rate: 1.0000e-04\n",
      "Epoch 10/15\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m288s\u001b[0m 2s/step - accuracy: 0.7867 - loss: 0.4654 - precision_1: 0.7787 - recall_1: 0.7483 - val_accuracy: 0.8360 - val_loss: 0.3575 - val_precision_1: 0.8222 - val_recall_1: 0.8203 - learning_rate: 1.0000e-04\n",
      "Epoch 11/15\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m288s\u001b[0m 2s/step - accuracy: 0.7867 - loss: 0.4654 - precision_1: 0.7787 - recall_1: 0.7483 - val_accuracy: 0.8360 - val_loss: 0.3575 - val_precision_1: 0.8222 - val_recall_1: 0.8203 - learning_rate: 1.0000e-04\n",
      "Epoch 11/15\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m287s\u001b[0m 2s/step - accuracy: 0.8002 - loss: 0.4543 - precision_1: 0.7876 - recall_1: 0.7736 - val_accuracy: 0.8582 - val_loss: 0.3414 - val_precision_1: 0.8731 - val_recall_1: 0.8088 - learning_rate: 1.0000e-04\n",
      "Epoch 12/15\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m287s\u001b[0m 2s/step - accuracy: 0.8002 - loss: 0.4543 - precision_1: 0.7876 - recall_1: 0.7736 - val_accuracy: 0.8582 - val_loss: 0.3414 - val_precision_1: 0.8731 - val_recall_1: 0.8088 - learning_rate: 1.0000e-04\n",
      "Epoch 12/15\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m290s\u001b[0m 2s/step - accuracy: 0.7914 - loss: 0.4599 - precision_1: 0.7817 - recall_1: 0.7575 - val_accuracy: 0.8402 - val_loss: 0.3564 - val_precision_1: 0.8619 - val_recall_1: 0.7765 - learning_rate: 1.0000e-04\n",
      "Epoch 13/15\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m290s\u001b[0m 2s/step - accuracy: 0.7914 - loss: 0.4599 - precision_1: 0.7817 - recall_1: 0.7575 - val_accuracy: 0.8402 - val_loss: 0.3564 - val_precision_1: 0.8619 - val_recall_1: 0.7765 - learning_rate: 1.0000e-04\n",
      "Epoch 13/15\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m291s\u001b[0m 2s/step - accuracy: 0.7928 - loss: 0.4516 - precision_1: 0.7827 - recall_1: 0.7598 - val_accuracy: 0.8614 - val_loss: 0.3360 - val_precision_1: 0.8686 - val_recall_1: 0.8226 - learning_rate: 1.0000e-04\n",
      "Epoch 14/15\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m291s\u001b[0m 2s/step - accuracy: 0.7928 - loss: 0.4516 - precision_1: 0.7827 - recall_1: 0.7598 - val_accuracy: 0.8614 - val_loss: 0.3360 - val_precision_1: 0.8686 - val_recall_1: 0.8226 - learning_rate: 1.0000e-04\n",
      "Epoch 14/15\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m319s\u001b[0m 2s/step - accuracy: 0.7983 - loss: 0.4468 - precision_1: 0.7818 - recall_1: 0.7782 - val_accuracy: 0.8519 - val_loss: 0.3488 - val_precision_1: 0.8789 - val_recall_1: 0.7857 - learning_rate: 1.0000e-04\n",
      "Epoch 15/15\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m319s\u001b[0m 2s/step - accuracy: 0.7983 - loss: 0.4468 - precision_1: 0.7818 - recall_1: 0.7782 - val_accuracy: 0.8519 - val_loss: 0.3488 - val_precision_1: 0.8789 - val_recall_1: 0.7857 - learning_rate: 1.0000e-04\n",
      "Epoch 15/15\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m284s\u001b[0m 2s/step - accuracy: 0.8041 - loss: 0.4349 - precision_1: 0.8073 - recall_1: 0.7534 - val_accuracy: 0.8561 - val_loss: 0.3428 - val_precision_1: 0.8548 - val_recall_1: 0.8272 - learning_rate: 1.0000e-04\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m284s\u001b[0m 2s/step - accuracy: 0.8041 - loss: 0.4349 - precision_1: 0.8073 - recall_1: 0.7534 - val_accuracy: 0.8561 - val_loss: 0.3428 - val_precision_1: 0.8548 - val_recall_1: 0.8272 - learning_rate: 1.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 13.\n",
      "Restoring model weights from the end of the best epoch: 13.\n",
      "\n",
      "[INFO] Phase 1 training completed\n",
      "\n",
      "[INFO] Phase 1 training completed\n"
     ]
    }
   ],
   "source": [
    "# Train the model - Phase 1: Train with frozen base model\n",
    "print(\"[INFO] Starting training - Phase 1: Frozen base model\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "history_phase1 = model.fit(\n",
    "    train_generator,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n[INFO] Phase 1 training completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b7523fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting fine-tuning - Phase 2: Unfreezing base model\n",
      "============================================================\n",
      "[INFO] Model recompiled for fine-tuning\n",
      "       >> Fine-tuning learning rate: 1e-05\n",
      "       >> Trainable layers: 9\n",
      "Epoch 1/10\n",
      "Epoch 1/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1210s\u001b[0m 10s/step - accuracy: 0.7405 - loss: 0.6123 - precision_4: 0.7276 - recall_4: 0.6954 - val_accuracy: 0.7259 - val_loss: 0.7251 - val_precision_4: 0.8995 - val_recall_4: 0.4539 - learning_rate: 1.0000e-05\n",
      "Epoch 2/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1210s\u001b[0m 10s/step - accuracy: 0.7405 - loss: 0.6123 - precision_4: 0.7276 - recall_4: 0.6954 - val_accuracy: 0.7259 - val_loss: 0.7251 - val_precision_4: 0.8995 - val_recall_4: 0.4539 - learning_rate: 1.0000e-05\n",
      "Epoch 2/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m798s\u001b[0m 7s/step - accuracy: 0.7497 - loss: 0.6154 - precision_4: 0.7294 - recall_4: 0.7236 - val_accuracy: 0.7989 - val_loss: 0.4932 - val_precision_4: 0.7319 - val_recall_4: 0.8871 - learning_rate: 1.0000e-05\n",
      "Epoch 3/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m798s\u001b[0m 7s/step - accuracy: 0.7497 - loss: 0.6154 - precision_4: 0.7294 - recall_4: 0.7236 - val_accuracy: 0.7989 - val_loss: 0.4932 - val_precision_4: 0.7319 - val_recall_4: 0.8871 - learning_rate: 1.0000e-05\n",
      "Epoch 3/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - accuracy: 0.7566 - loss: 0.5957 - precision_4: 0.7561 - recall_4: 0.7148\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m536s\u001b[0m 5s/step - accuracy: 0.7603 - loss: 0.5764 - precision_4: 0.7456 - recall_4: 0.7259 - val_accuracy: 0.6857 - val_loss: 0.7143 - val_precision_4: 0.9102 - val_recall_4: 0.3502 - learning_rate: 1.0000e-05\n",
      "Epoch 4/10\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m536s\u001b[0m 5s/step - accuracy: 0.7603 - loss: 0.5764 - precision_4: 0.7456 - recall_4: 0.7259 - val_accuracy: 0.6857 - val_loss: 0.7143 - val_precision_4: 0.9102 - val_recall_4: 0.3502 - learning_rate: 1.0000e-05\n",
      "Epoch 4/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m517s\u001b[0m 4s/step - accuracy: 0.7664 - loss: 0.5638 - precision_4: 0.7513 - recall_4: 0.7345 - val_accuracy: 0.8423 - val_loss: 0.3675 - val_precision_4: 0.8322 - val_recall_4: 0.8226 - learning_rate: 5.0000e-06\n",
      "Epoch 5/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m517s\u001b[0m 4s/step - accuracy: 0.7664 - loss: 0.5638 - precision_4: 0.7513 - recall_4: 0.7345 - val_accuracy: 0.8423 - val_loss: 0.3675 - val_precision_4: 0.8322 - val_recall_4: 0.8226 - learning_rate: 5.0000e-06\n",
      "Epoch 5/10\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m504s\u001b[0m 4s/step - accuracy: 0.7775 - loss: 0.5166 - precision_4: 0.7615 - recall_4: 0.7506 - val_accuracy: 0.8434 - val_loss: 0.3660 - val_precision_4: 0.8043 - val_recall_4: 0.8710 - learning_rate: 5.0000e-06\n",
      "Epoch 5: early stopping\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m504s\u001b[0m 4s/step - accuracy: 0.7775 - loss: 0.5166 - precision_4: 0.7615 - recall_4: 0.7506 - val_accuracy: 0.8434 - val_loss: 0.3660 - val_precision_4: 0.8043 - val_recall_4: 0.8710 - learning_rate: 5.0000e-06\n",
      "Epoch 5: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "[INFO] Phase 2 fine-tuning completed\n",
      "\n",
      "[INFO] Phase 2 fine-tuning completed\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning: Unfreeze the base model for better performance\n",
    "print(\"[INFO] Starting fine-tuning - Phase 2: Unfreezing base model\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Unfreeze the base model\n",
    "base_model.trainable = True\n",
    "\n",
    "# Freeze the first 100 layers to retain low-level features\n",
    "for layer in base_model.layers[:100]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Recompile with a lower learning rate for fine-tuning\n",
    "fine_tune_lr = LEARNING_RATE / 10\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=fine_tune_lr),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    ")\n",
    "\n",
    "print(f\"[INFO] Model recompiled for fine-tuning\")\n",
    "print(f\"       >> Fine-tuning learning rate: {fine_tune_lr}\")\n",
    "print(f\"       >> Trainable layers: {sum([1 for layer in model.layers if layer.trainable])}\")\n",
    "\n",
    "# Continue training with unfrozen layers\n",
    "history_phase2 = model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n[INFO] Phase 2 fine-tuning completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "695f2fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Evaluating model on test dataset\n",
      "============================================================\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 2s/step - accuracy: 0.7990 - loss: 0.5114 - precision_4: 0.9298 - recall_4: 0.6085\n",
      "\n",
      "[INFO] Test Results:\n",
      "       >> Test Loss: 0.5114\n",
      "       >> Test Accuracy: 0.7990\n",
      "       >> Test Precision: 0.9298\n",
      "       >> Test Recall: 0.6085\n",
      "       >> F1-Score: 0.7356\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 2s/step - accuracy: 0.7990 - loss: 0.5114 - precision_4: 0.9298 - recall_4: 0.6085\n",
      "\n",
      "[INFO] Test Results:\n",
      "       >> Test Loss: 0.5114\n",
      "       >> Test Accuracy: 0.7990\n",
      "       >> Test Precision: 0.9298\n",
      "       >> Test Recall: 0.6085\n",
      "       >> F1-Score: 0.7356\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the trained model on test data\n",
    "print(\"[INFO] Evaluating model on test dataset\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_results = model.evaluate(test_generator, verbose=1)\n",
    "test_loss = test_results[0]\n",
    "test_accuracy = test_results[1]\n",
    "test_precision = test_results[2]\n",
    "test_recall = test_results[3]\n",
    "\n",
    "print(\"\\n[INFO] Test Results:\")\n",
    "print(f\"       >> Test Loss: {test_loss:.4f}\")\n",
    "print(f\"       >> Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"       >> Test Precision: {test_precision:.4f}\")\n",
    "print(f\"       >> Test Recall: {test_recall:.4f}\")\n",
    "print(f\"       >> F1-Score: {2 * (test_precision * test_recall) / (test_precision + test_recall):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6d7eb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Model saved successfully to: exercise_6_trained_resnet.h5\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "MODEL_SAVE_PATH = 'exercise_6_trained_resnet.h5'\n",
    "\n",
    "model.save(MODEL_SAVE_PATH)\n",
    "print(f\"[INFO] Model saved successfully to: {MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47f75ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Prediction function defined successfully\n"
     ]
    }
   ],
   "source": [
    "# Image prediction function\n",
    "def predict_image(img_path, model_path='exercise_6_trained_resnet.h5', img_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    Predict whether an image is a muffin or chihuahua using the trained ResNet model.\n",
    "    \n",
    "    Args:\n",
    "        img_path (str): Path to the image file\n",
    "        model_path (str): Path to the saved model file\n",
    "        img_size (tuple): Size to resize the image to (height, width)\n",
    "    \n",
    "    Returns:\n",
    "        None: Prints the prediction result\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the trained model\n",
    "        loaded_model = tf.keras.models.load_model(model_path)\n",
    "        \n",
    "        # Load and preprocess the image\n",
    "        img = image.load_img(img_path, target_size=img_size)\n",
    "        img_array = image.img_to_array(img)\n",
    "        img_array = img_array / 255.0\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        \n",
    "        # Make prediction\n",
    "        prediction = loaded_model.predict(img_array, verbose=0)[0, 0]\n",
    "        \n",
    "        # Interpret prediction (assumes class indices: {chihuahua: 0, muffin: 1})\n",
    "        label = \"Chihuahua\" if prediction >= 0.5 else \"Muffin\"\n",
    "        confidence = prediction if prediction >= 0.5 else (1 - prediction)\n",
    "        \n",
    "        print(f\"[PREDICTION] {img_path}\")\n",
    "        print(f\"             >> Result: {label}\")\n",
    "        print(f\"             >> Confidence: {confidence:.2%}\")\n",
    "        print(f\"             >> Raw score: {prediction:.4f}\\n\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"[ERROR] Image file not found: {img_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Prediction failed for {img_path}: {str(e)}\")\n",
    "\n",
    "print(\"[INFO] Prediction function defined successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4970b4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Running predictions on test images\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PREDICTION] archive/predict/predict_1.png\n",
      "             >> Result: Muffin\n",
      "             >> Confidence: 95.77%\n",
      "             >> Raw score: 0.0423\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PREDICTION] archive/predict/predict_2.png\n",
      "             >> Result: Muffin\n",
      "             >> Confidence: 87.59%\n",
      "             >> Raw score: 0.1241\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PREDICTION] archive/predict/predict_3.png\n",
      "             >> Result: Muffin\n",
      "             >> Confidence: 96.44%\n",
      "             >> Raw score: 0.0356\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PREDICTION] archive/predict/predict_4.png\n",
      "             >> Result: Muffin\n",
      "             >> Confidence: 99.82%\n",
      "             >> Raw score: 0.0018\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002317488D1C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002317488D1C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PREDICTION] archive/predict/predict_5.png\n",
      "             >> Result: Muffin\n",
      "             >> Confidence: 99.52%\n",
      "             >> Raw score: 0.0048\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000023174140360> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000023174140360> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PREDICTION] archive/predict/predict_6.png\n",
      "             >> Result: Muffin\n",
      "             >> Confidence: 99.77%\n",
      "             >> Raw score: 0.0023\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PREDICTION] archive/predict/predict_7.png\n",
      "             >> Result: Chihuahua\n",
      "             >> Confidence: 55.55%\n",
      "             >> Raw score: 0.5555\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PREDICTION] archive/predict/predict_8.png\n",
      "             >> Result: Muffin\n",
      "             >> Confidence: 97.99%\n",
      "             >> Raw score: 0.0201\n",
      "\n",
      "[INFO] All predictions completed\n"
     ]
    }
   ],
   "source": [
    "# Test predictions on sample images\n",
    "print(\"[INFO] Running predictions on test images\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "predict_image(\"archive/predict/predict_1.png\")\n",
    "predict_image(\"archive/predict/predict_2.png\")\n",
    "predict_image(\"archive/predict/predict_3.png\")\n",
    "predict_image(\"archive/predict/predict_4.png\")\n",
    "predict_image(\"archive/predict/predict_5.png\")\n",
    "predict_image(\"archive/predict/predict_6.png\")\n",
    "predict_image(\"archive/predict/predict_7.png\")\n",
    "predict_image(\"archive/predict/predict_8.png\")\n",
    "\n",
    "print(\"[INFO] All predictions completed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
